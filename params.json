{"name":"Pml-course-project","tagline":"In this repository I upload the files correponding to the PML course project","body":"---\r\ntitle: \"Classifying weight lifting stiles\"\r\n\r\ndate: \"Friday, November 21, 2014\"\r\noutput: html_document\r\n---\r\nA predictive analysis to classify weight lifting styles using body sensors data \r\n------------------------------------------------------------------------------------\r\n\r\nAs you know, dear colleague, this is the report where I describe all the analysis implemented in order to classify and predict the weight lifting style of 6 subjects who performed this exercise in 5 different ways and wearing body sensor which registered the data this analysis is based on. The whole process is described in detail (I hope), the aim is to show it step by step and drive through all the choices that have been made to solve the problem.\r\n\r\nI start loading the basic training dataset\r\n\r\n````{r}\r\n#downloading and loading training ds\r\nsetwd (\"C:/Users/stefano/Dropbox/cursera/Practical Machine Learning/Course Project\")\r\ntraining <- read.csv(\"./data/training.csv\")\r\n```\r\n\r\nI also load all the packages I will need\r\n````{r}\r\nlibrary(caret)\r\nlibrary(rpart)\r\nlibrary(randomForest)\r\n````\r\n\r\nThe dependent variable of my model, the one I want to predict, is the lifting style of the subjects. There are 5 different classes of weigth lifting.\r\n````{r}\r\n#Table of the classe variable\r\nround(prop.table(table(training$classe)), digits=2)\r\n````\r\n\r\n\r\n**Data procesing**\r\n\r\nI observe that several variables read as factors, are actually numeric. The reason they're read as factors is that they present some missing values not recognised as such by R, in particular they have a lot of \"\" and \"#DIV/0!\".\r\n\r\nI do something that migth not seem odd in a course, but it is everyday practice when dealing with large dataset: I \"clean\" the dataset removing \"\" and \"#DIV/0!\" before loading them in R (I just do it in a normal Notepad)\r\nI'm sorry if this is not reproducible, but I give my word it's not a trick ;) just a regular way of processing data\r\n\r\n````{r}\r\ntraining <- read.csv(\"./data/training_clean2.csv\")\r\n```\r\n\r\nSome further data processing is needed. \r\nHere are some variables I wish to remove:\r\nX is just an id\r\nuser_name coluld lead to overfitting\r\ntime could also lead to overfitting\r\nsome other variables loaded as logic vectors\r\n\r\n````{r}\r\ntraining <- training[,!(names(training) %in% c(\"X\", \"user_name\", \"kurtosis_yaw_belt\", \"skewness_yaw_belt\", \"kurtosis_yaw_dumbbell\", \"skewness_yaw_dumbbell\",\r\n                         \"kurtosis_yaw_forearm\", \"skewness_yaw_forearm\", \"cvtd_timestamp\", \"new_window\"))]\r\n\r\n```\r\n\r\nFurthermore I want to remove all variables that seem to represent more \"noise\" than \"signal\". For this reason I drop all variables with more than 95% of NA values.\r\n````{r}\r\ntraining <- training[,colSums(is.na(training))<0.95*nrow(training)]\r\n```\r\n\r\nAt this point I'm handling a dataset of 56 variables. We can do better and further reduce the number of variables (almost) without loosing information. I can apply Principal Component Analysis. Indeed, as you can see below, some of the variables are correlated.\r\n\r\n````{r}\r\nCor <- abs(cor(training[,-56]))\r\ndiag(Cor) <- 0 #set diagonal values to 0\r\nwhich(Cor > 0.8, arr.ind=T)#I see that there are quite a lot highly correlated numerical variables\r\npp <- preProcess(training[,-56], method=\"pca\", thresh = 0.99)\r\ntrainingPC <- predict(pp,training[,-56] )#from 56 to 38 variables keeping 99% of the variance\r\ntrainingPC <- cbind(trainingPC, training$classe)\r\nnames(trainingPC) <- c(names(trainingPC)[1:38], \"classe\")\r\n```\r\n\r\nAfter the data processing I end up with a dataset of 39 variables (the variable I want to predict and 38 predictors) out of 160.\r\n\r\nA step to reduce the number of cases on which the model will be trained. The initial 19622 cases slow the proces quite a bit. \r\n\r\n````{r}\r\nset.seed(1)\r\ntrainingPC_sm <- trainingPC[sample(nrow(trainingPC), dim(trainingPC)[1]*.30), ]# 30% sample\r\n````\r\n\r\nFinally I split the dataset into 2 components (70% training, 30% testing)\r\n````{r}\r\nset.seed(2)\r\ninTrain = createDataPartition(trainingPC_sm$classe, p = 0.7)[[1]]\r\ntrainingPC_sm_tr = trainingPC_sm[ inTrain,]\r\ntrainingPC_sm_ts = trainingPC_sm[-inTrain,]\r\n````\r\n\r\n\r\n\r\n**How I built the model**\r\n\r\nTime to train models!\r\nI will try 3 different models.The first is a simple tree. The second is a tree with some data processing and a cross validation. The third will be a random forest. \r\n\r\n````{r}\r\ntree1 <- train(classe~., method=\"rpart\", data=trainingPC_sm_tr)\r\ntree2 <- train(classe~., method=\"rpart\", preProcess=c(\"center\", \"scale\"), trControl=trainControl(method = \"cv\", number = 4), data=trainingPC_sm_tr)\r\nforest <- randomForest(classe~., importance=TRUE, method=\"rf\",data=trainingPC_sm_tr,ntree=500)\r\n````\r\n\r\n\r\nTo understand how these three model perform I will look at their Accuracy. First at the in-training accuracy\r\n\r\n````{r}\r\n#In training predictions\r\ntree1pred <- predict(tree1, newdata=trainingPC_sm_tr)\r\ntree2pred <- predict(tree2, newdata=trainingPC_sm_tr)\r\nforestpred <- predict(forest, newdata=trainingPC_sm_tr)\r\nconfusionMatrix(tree1pred, trainingPC_sm_tr$classe)\r\nconfusionMatrix(tree2pred, trainingPC_sm_tr$classe)\r\nconfusionMatrix(forestpred, trainingPC_sm_tr$classe)\r\n````\r\n\r\nWell, both trees seem to work the same way. They actually have the same (low) accuracy: 37.2%\r\nThe forest is doing pretty well, a 100% of in-training accuracy.\r\n\r\n**Expected out of the sample error**\r\n\r\nWe know that in-training accuracy is not a good measure. Therefore I test the models on a testing set.\r\n\r\n````{r}\r\n#In training.testing predictions\r\ntree1pred.te <- predict(tree1, newdata=trainingPC_sm_ts)\r\ntree2pred.te <- predict(tree2, newdata=trainingPC_sm_ts)\r\nforestpred.te <- predict(forest, newdata=trainingPC_sm_ts)\r\nconfusionMatrix(tree1pred.te, trainingPC_sm_ts$classe)\r\nconfusionMatrix(tree2pred.te, trainingPC_sm_ts$classe)\r\nconfusionMatrix(forestpred.te, trainingPC_sm_ts$classe)\r\n````\r\n\r\nBoth trees are definitely the same, with an out of sample Accuracy of 36.3%.\r\nThey are not able to predict classe B and C.\r\nThe Random Forest estimation is predicting definitely better with an accuracy of 92.3% and a Kappa of 90%.\r\n\r\nThe random forest is the chosen prediction.\r\n\r\n**Final prediction**\r\n\r\nFinally I apply the model to the test set we were given and do my prediction.\r\nI need to load and transform the test dataset the same way I did with the training one, with a particular attention to the PCA analysis.\r\n\r\n````{r}\r\ntest <- read.csv(\"./data/test_clean.csv\")\r\ntest <- test[,!(names(test) %in% c(\"X\", \"user_name\", \"kurtosis_yaw_belt\", \"skewness_yaw_belt\", \"kurtosis_yaw_dumbbell\", \"skewness_yaw_dumbbell\",\r\n                                               \"kurtosis_yaw_forearm\", \"skewness_yaw_forearm\", \"cvtd_timestamp\", \"new_window\"))]\r\ntest <- test[,colSums(is.na(test))<0.95*nrow(test)]\r\ntestPC <- predict(pp,test[,-56] )\r\n````\r\n\r\nAND MY PREDICTION IS\r\n\r\n\r\n````{r}\r\nprint(predict(forest, newdata=testPC))\r\n````\r\n\r\n**THANK YOU**\r\n\r\n...and sorry for typos...\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}